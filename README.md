# NanoGPT-trial
 Trying out Andrej Karpathys GPT module

Coded a basic Generatively Pretrained Transformer (GPT) model inspired by the "Attention is All You Need" paper, drawing from OpenAI's GPT-2 and GPT-3 frameworks.
Finetuned the model on a Shakespeare Prose dataset, using it in order to generate more text for the same
